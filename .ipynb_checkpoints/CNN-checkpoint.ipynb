{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "Las \"Convolutional Neural Networks - CNN\" o Redes Neuronales Convolucionales en español, son un tipo de red neuronal enfocada en el procesamieno de matrices bidimencionales, debido a esto son muy utlizadas para visión artifcial, clasificación y segmentación de imágenes.\n",
    "\n",
    "Las CNN, realizan en general el mismo funcionamiento de un \"Neural Network\", ya que primeramente realizan la extracción de características para posteriormente realizar el proceso de decisión.\n",
    "\n",
    "\"Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers\"\n",
    "\n",
    "## Convolución ##\n",
    "\n",
    "Lo relevante hablando de estas es como opera para realizar esto, pues el primer paso es realizar convoluciones.\n",
    "\n",
    "Para esto planteemos el ejemplo de imágenes.\n",
    "\n",
    "Cada imagen \"i\" fungirá como nuestra entrada, más precisamente $ I = \\{i_{1} ... i_{N} \\} $ .\n",
    "\n",
    "Una imagen es una matriz bidimensional, en donde cada uno de sus elementos cuenta con un valor del 0 al 255.\n",
    "\n",
    "Y a su vez tenemos un Kernel, tambien llamado filtro una vez lo operamos sobre toda la imagen.\n",
    "\n",
    "Un kernel es una matriz bidimencional que se utliza para realizar enfoques, desenfoques, detección de bordes, realce, entre otras aplicaciones.\n",
    "\n",
    "\n",
    "Lo que nos es muy útil si lo que buscamos es obtener caracteristicas de una imagen.\n",
    "\n",
    "Para operar el kernel sobre una imagen es necesario realizar una convolución entre ambos.\n",
    "\n",
    "Recordando la definición de convolución continua es la siguiente:\n",
    "$$ s(t) = \\int x(a)w(t-a)\\,da $$\n",
    "\n",
    "$$ s(t) = (x * w)(t) $$\n",
    "\n",
    "Donde w es función de densidad de probabilidad, y w es 0 para los valores negativos\n",
    "\n",
    "Referente a nuestro problema, x es el ***input***, w es el ***kernel*** y s es lo llamado ***feauture map***\n",
    "\n",
    "Pero esta definición de convolución no es útil para nosotros ya que nuestros datos son discretos.\n",
    "\n",
    "Para esto existe la definición discreta de convolución:\n",
    "$$ s(t) = (x * w)(t) = \\sum_{a=-\\infty}^{\\infty} x(a)w(t-a)$$\n",
    "\n",
    "Aún sobre esto, en esta y muchas aplicaciones de machine learning el ***input*** es una matriz multidimensional de **datos**, a su vez el ***kernel*** es una matriz multidimensional de ***parámetros***, dicho esto, en terminología adaptada para los algoritmos de aprendizaje, a estas matrices multidimensionales se les llama ***tensors***, a su vez asuminos que su valor es 0 excepto en el conjunto finito de puntos que los conforman.\n",
    "\n",
    "La razón de esta última declaración es debido a que la convolución discreta está definda de $-\\infty\\ a\\ \\infty$ por lo que resultaría imposible operar con esta, sin definir el complemento del conjunto de puntos de los ***tensores***, debido a esto se definío al conjunto complemento como 0 en todos sus elemtos.\n",
    "\n",
    "Con esto en mente haremos uso de la convolución en más de una dimensión como ejes.\n",
    "\n",
    "Para ejemplificar usando el ejemplo anterior\n",
    "$$ S(i,j) = ( I * K)(i,j) = \\sum_{m} \\sum_{n} I(m,n)K(i-m,j-n) $$\n",
    "\n",
    "$$Aplicando\\ la\\ propiedad\\ conmutativa\\ de\\ la\\ convoución$$\n",
    "\n",
    "$$ S(i,j) = ( K * I)(i,j) = \\sum_{m} \\sum_{n} I(i-m,j-n)K(m,n) $$\n",
    "\n",
    "\n",
    "Usualmente la segunda forma es mayormente usada para la implemetación de las bibliotecas de ML, debido a que hay menos variación en el rango de valores válidos para m y n.\n",
    "\n",
    "La propieda conmutativa de la convolución surgío porque volteamos el kernel relativo al input, aunque esto únicamente tiene relevancia para la implementación, puesto que para el analisis de la CNN esto es irrelevante.\n",
    "\n",
    "Pese a esta propiedad, muchas otras bibliotecas hacen uso de una operación llamada ***cross-correlation*** la cual es parecida a la convolución, tanto que incluso es llamada de la mis forma, esta es:\n",
    "$$ S(i,j) = ( K * I)(i,j) = \\sum_{m} \\sum_{n} I(i+m,j+n)K(m,n) $$\n",
    "\n",
    "Para una representación gráfica del comportamiento de la convolución:\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"float: left;text-align: center;width: 50%;\"> <img src=\"conv1.png\" width=\"70%\"/>  </div>\n",
    "<div style=\"float: right;text-align: center;width: 50%;position: relative;\"> <img src=\"conv2.png\" width=\"80%\"/></div>\n",
    "</div>\n",
    "\n",
    "La convolución ofrece un menor número de operaciones y una ventaja respecto al espacio, ya que debido a que la interacción entre el ***input*** y el ***kernel*** es sumamente menor respecto a la clásica multiplicación de matrices, y debido a que kernels pequeños relativos al tamaño de la entrada son los utilizados el  costo respecto al almacenamiento es menor.\n",
    " \n",
    "\n",
    "### Interacción de la red ###\n",
    "\n",
    "En las CNN uno de los principios que se sobre pone a todo es *sparce interaction* o **escas iteraccion**, esto se ve reflejado en la interacción entre los nodos de las diferentes capas, ya que en estas la interacción *directa* que tiene la salida de un nodo $X_{n}^{l}$ donde *l* es la capa y *n* el nodo de la dicha capa, únicamente afectará **directamente** a los nodos $X_{n-1}^{l+1}$, $X_{n}^{l+1}$ y $X_{n+1}^{l+1}$ esto pensando que el kernel tiene un ***tamaño de 3***, esto promueve la escasa interacción.\n",
    "\n",
    "Pero debido a esto se podría a llegar a pensar \"¿Cómo obtienen toda la información que le ofrece la entrada si la interacción directa es tan poca?\", pues la solución a esto se plantea que se debe a la interacción **indirecta** de los nodos, ya que el **receptive field** crece conforme las capas se vuelvan mas profundas pudiendo tener interacción con todo los nodos.\n",
    "\n",
    "Para una representación gráfica del comportamiento de lo anterior:\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"float: left;text-align: center;width: 50%;\"> <img src=\"receptive_field.png\" width=\"50%\"/>  </div>\n",
    "<div style=\"float: right;text-align: center;width: 50%;\"> <img src=\"receptive_field2.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "Cabe mencionar que una de las mayores ventajas tanto de la convolución como de la escasa interacción es que ofrece una disminución en las dimensiones de los datos conforme pasa a través de las capas.\n",
    "\n",
    "### Parámetros compartidos ###\n",
    "En una red neuronal regular los ***parámetros de aprendizaje*** son llamados vectores de pesos, dando como resultdo una matriz de pesos por capa, donde esta matriz tiene un tamaño de $(W^{m * n})^{l}$, esto únicamente para la capa **l**, esto implica que existirán **L** matrices para realizar las operaciones pertinentes, el problema de esto recae en el número de usos que se le da a cada peso $w_{p,q}^{l}$, ya que únicamente tienen un uso para la generación de la salida respecto a los nodos de entrada.\n",
    "\n",
    "En las CNN esto es diferente ya que se hace uso de ***parameter sharing***, esto es debido a que en cada capa se hace uso de un kernel (filtro), para obtener las características necesarias, pero esto ofrece muchas ventajas entre las cuales está que únicamente se almacena una matriz mucho más pequeña que las anteriores.\n",
    "\n",
    "### Equivariance ###\n",
    "Debido al comportamiento particular que causa el ***parameter sharing*** en las CNN, las capas que las conforman desarrollan una propiedad llamada ***equivariance*** a las transformaciones.\n",
    "\n",
    "Que una función es *equivariance* significa que si el *input* cambia, el *output* cambia en la misma dirección. Específicamente una $f(x)$ es *equivariance* a una función g, si $f(g(x)) = g(f(x))$.\n",
    "\n",
    "En el sentido de la convolución si g es una función que transforma su input , f es la convulución, x el input y k el kernel, $g(f(x,k)) = f(g(x),k)$,\n",
    "$$ si Y =  g(X * K) = g(X) * K $$\n",
    "\n",
    "La convolución es parte únicamente de las primeras capas de las CNN, estas enfocadas a obtención de características\n",
    "\n",
    "## Pooling ##\n",
    "\n",
    "Para describir el comportamiento del ***pooling*** es sumamente útil, primeramente describir la composición típica de una capa en una CNN.\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"text-align: center;\"> <img src=\"poling.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "Una explicación breve sobre estas etapas sería la siguiente:\n",
    "+ Primera etapa: En esta estapa se realizan el conjunto de convoluciones a las entradas de forma paralela para producir un **conjunto de  activaciones lineares**\n",
    "+ Segunda etapa: En esta etapa cada activación lineal es pasada a través de una función de activación no-lineal, como la función de activación de rectificado lineal de su acrónimo en inglés (**ReLu**)\n",
    "    $$ ReLU, f(x) = x^{+} = max(0,x)  $$\n",
    "Una vez llegando a la tercera etapa, procederemos a explicar la función ***pooling***, esta función transforma la salida del proceso anterior en una ubicación determinada con un resumen de estadísticas de la salidas cercanas.\n",
    "\n",
    "Por ejemplo: \n",
    "El ***Max pooling*** en el resumen estadístico regresa el valor máximo de los vecinos de una área cuadrangular.\n",
    "\n",
    "Pero en lo general **pooling** ayuda a realizar una representación aproximada **invariante** a una transformación pequeña de lo que entro.\n",
    "\n",
    "Una transformación invariante significa, que si transformamos el input en una pequeña cantidad, muchos de los valores resultantes de la funcion no cambiaran\n",
    "\n",
    "Ya que es mas importante conocer las caracteristicas que la localización de ellas.\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"text-align: center;\"> <img src=\"pooling.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "Otra de las funciones destras del ***poolling*** es cuando el numero de parametrso de la siguinete capa esta en funcion del tamaño de su entrada (como en caso de que esa capa este completamente conectada y este basada en la multiplicacion de matrices), ya que el poolling reducira el tamaño de su entrada  por lo que mejorara su eficiencia de procesamiento y reduce los requerimientos de memoria para almacenar los parametros.\n",
    "\n",
    "A su vez el poolling funciona para ajustar los datos de entrada a el tamaño necesario por las arquitecturas de la redes neuronales\n",
    "\n",
    "## Variante de Funcion de Convolucion ##\n",
    "\n",
    "Una de las razones de no usar la definicion literaria de Convolucion discreta es que sera realizada en paralelo, ya que la convolucion con un solo ***Kernel*** o filtro solo podria obtener un tipo de caracteristicas, y usualmente querriamos obtener varias caracteristicas en la misma capa.\n",
    "\n",
    "Otra razon es que usualmente los datos de entrada no seran matrices con valores reales, si no que seran matrices con valores vectoriales, como podria ser por ejemplo una imagen a color donde cada elemento tendria un vector de 3 sub elementos que describen la consentracion de rojo, verde y azul.\n",
    "\n",
    "Y recordemos que una red convolucional tiene como entrada en una capa oculta, el resultado de un conjunto de convoluciones, por lo que se necesitaria un sub proceso para pocesar una matriz con mas dimensiones, si mantuvieramos la defincion descrita anteriormente.\n",
    "\n",
    "Por lo que por ejemplo para trabajar con una imagen se concidera que tendremos como entrada y una salida un tensor 3-D, con un indice para indicar los canales y 2 indices para indicar la localizacion del pixel en la sub-imagen.\n",
    "\n",
    "Pensando en dar una explicacion de como las bibliotecas computacionales funcionan, nos gustaria comentarles que estas trabajan con tensores 4-D, ya que ellos trabajon por lotes, es decir que el cuato indice indica el lote en el que nos encontramos\n",
    "\n",
    "Otra de la cosas que nos debemos encargar al utilizar una variante de la funcion, es asegurarnos de que cuente con la propiedad conmutativa. Para que cuente con esta dicha propiedad, la entrada y la salida deben de tener el mismo numero de canales.\n",
    "\n",
    "Para esto, asumimos que tenemos un tensor ***kernel*** 4-D **K** con elementos $K_{i,j,k,l}$ dando una conexion estrecha entre una unidad ***i*** en el canal de la salida y una unidad ***j*** en el canal de entrada, con un offset (tamaño de las filas y columnas del kernel) de ***k*** filas y de ***l*** columnas entre la unidad de salida y la de entrada. Tambien tomamos como la entrada de datos a ***V*** como a sus elementos $V_{i,j,k}$ donde el canal es ***i***, las filas ***j*** y las columnas ***k***.Y consideramos la salida como ***Z*** con el mismo formato que **V**. \n",
    "\n",
    "Dado todo lo anterior la variante de la convlucion es la siguiente:\n",
    "\n",
    "$$Z_{i,j,k} = \\sum_{l,m,n} V_{l,j+m-1,k+n-1}K_{i,l,m,n} $$\n",
    "\n",
    "\n",
    "Nota: el \"-1\" en la sumatorio surge porque el primer elemeto de los arreglos esta marcado como ***1***\n",
    "\n",
    "Si buscamos reducir el costo computacional de la operacion arriesgandonos a no poder obtener las caracteristicas, lo que se puede realizar es un downsampling, o submuestre de la salida de convolucion. Para es unicamente guardamos los valores de cada ***s*** strides o pasos, esto se mostraria de la siguiente forma.\n",
    "\n",
    "$$Z_{i,j,k} = c(K,V,s)_{i,j,k} = \\sum_{l,m,n} [V_{l,(j-1)x s+m,(k-1)x s+n} K_{i,l,m,n}] $$\n",
    "\n",
    "otras variantes de la convolucion podrian ser ***unshared convolution***:\n",
    "En esta variante no se hace uso tal cual de la convolucion con un kernel, sino con una matriz de pesos en una ***MLP***:\n",
    "\n",
    "Para una matriz de pesos ***W*** 6-D, $W_{i,j,k,l,m,n}$, donde ***i*** es el canal de salida, ***j*** la fila de salida, ***k*** la columna de salida, ***l*** el canal de entrada, ***m*** el offset de filas respecto al input y ***n*** el offset de columnas respecto al input.\n",
    "\n",
    "$$ Z_{i,j,k} = \\sum_{l,m,n} [V_{l,j+m-1,k+n-1} W_{i,j,k,l,m,n}]   $$\n",
    "\n",
    "## Aprendizaje ##\n",
    "Para mejorar las predicciones, se debe realizar mejoras en el proceso de seleccion de caracteristicas, por lo que esto implica mejorar los ***kernels*** utilizados, la razon de esto es que como se comento anteriormente cada tipo de kernel esta enfocado en obtener un tipo de caracteristicas, esto realizando transformaciones de los datos, para de esta forma obtener una representacion de los datos donde la caracteristica buscada se haga presente de forma más evidente, tambien se puede abstraer como el proceso de eliminar el ruido de la informacion, para unicamente mantener la carcateristica buscada por el Kernel.\n",
    "\n",
    "Un ejemplo de kernel seria el Kernel Gaussiano, el cual se pude ver el resultado de su aplicacion en la siguiente imagen:\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"text-align: center;\"> <img src=\"Gaus.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "Retomando el tema del proceso para mejorar la obtencion de caracteristicas el cual consiste en modificar los kernels para refinarlos y de esta forma obtenerlos de forma más precisa, se obtiene el gradiente respecto a la salida.\n",
    "\n",
    "En muchos casos este gradiente se puede obtener usando directamente la operacion de convolucion, pero a su vez en muchos casos no, incluyendo el caso en el que se hace uso de ***stride***. Como recordaremos la convolucion es una operacion lineal y esta puede ser descrita como una multiplicacion matricial (Si primero modificamos la forma del tensor de entrada como un vector), la matriz debera de ser dispersa. \n",
    "\n",
    "Una vez con esta representacion de vector o con el tensor y la funcion de convolucion, se procede a realizar el procedimiento similar al de back propagation como en una red totalmente conectada.\n",
    "\n",
    "\n",
    "Para ejemplificar el proceso de obtencion del gradiente, supondremos que queremos entrenar una CNN que incorpora ***stride*** con un conjunto de ***kernels*** representado por ***K***, aplicado a una imagen multicanal (Imagen a color) ***V*** con un stride *s* definido como $c(K,V,s)$, es decir esta operacion:\n",
    "\n",
    "$$Z_{i,j,k} = c(K,V,s)_{i,j,k} = \\sum_{l,m,n} [V_{l,(j-1)x s+m,(k-1)x s+n} K_{i,l,m,n}] $$\n",
    "\n",
    "Ya que nosotros buscamos minimizar la funcion de costo en este caso $J(V,K)$, donde mediante back-propagation obtendremos un tensor ***G***, descrito de la siguiente forma.\n",
    "    $$ G_{i,j,k} = \\frac{\\partial}{\\partial Z_{i,j,k}} J(\\textbf{V},\\textbf{K}) $$\n",
    "\n",
    "Para entrenar la red necesitamos computar la siguiente derivada respecto los pesos de los kernels:\n",
    "\n",
    "$$ g(\\textbf{G},\\textbf{V},s)_{i,j,k,l} = \\frac{\\partial}{\\partial K_{i,j,k,l}} J(\\textbf{V},\\textbf{K}) = \\sum_{m,n} [G_{i,m,n} V_{j,(m-1)x s+k,(n-1)x s+l}]  $$\n",
    "\n",
    "Si esta capa no es la capa inferior de la red, necesitaremos calcular el gradiente con respecto a ***V*** para propagar el error hacia atrás (input). Para hacerlo, podemos usar la siguiente funcion:\n",
    "$$ h(\\textbf{K},\\textbf{G},s)_{i,j,k} = \\frac{\\partial}{\\partial V_{i,j,k}} J(\\textbf{V},\\textbf{K}) \\\\ = \\sum_{l,m \\\\ s.t. \\\\ (l-1)xs+m=j }\\ \\sum_{n,p \\\\ s.t. \\\\ (n-1)xs+p=k } \\sum_{q} K_{q,i,m,p} G_{q,l,n}$$\n",
    "\n",
    "Una vez descritas todas las capas y caracteristicas anteriores como fueron la ***convolucion***, el ***poolling***, las variantes de la convolucion y la variante necesaria para aplicar back-propagation, finalmente podemos ilustrarles una arquitectura de una CNN sencilla.\n",
    "\n",
    "<div >\n",
    "<div style=\"text-align: center;\"> <img src=\"arch.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "[Ejemplo escrito usando Keras](./EG_Keras.ipynb)\n",
    "[Ejemplo escrito usando PyTorch](./EG_Pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
