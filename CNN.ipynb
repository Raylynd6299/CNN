{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "Las \"Convolutional Neural Networks - CNN\" o Redes Neuronales Convolucionales en español, son un tipo de red neuronal enfocada en el procesamieno de matrices bidimencionales, debido a esto son muy utlizadas para visión artifcial, clasificación y segmentación de imágenes.\n",
    "\n",
    "Las CNN, realizan en general el mismo funcionamiento de un \"Neural Network\", ya que primeramente realizan la extracción de características para posteriormente realizar el proceso de decisión.\n",
    "\n",
    "\"Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers\"\n",
    "\n",
    "## Convolución ##\n",
    "\n",
    "Lo relevante hablando de estas es como opera para realizar esto, pues el primer paso es realizar convoluciones.\n",
    "\n",
    "Para esto planteemos el ejemplo de imágenes.\n",
    "\n",
    "Cada imagen \"i\" fungirá como nuestra entrada, más precisamente $ I = \\{i_{1} ... i_{N} \\} $ .\n",
    "\n",
    "Una imagen es una matriz bidimensional, en donde cada uno de sus elementos cuenta con un valor del 0 al 255.\n",
    "\n",
    "Y a su vez tenemos un Kernel, tambien llamado filtro una vez lo operamos sobre toda la imagen.\n",
    "\n",
    "Un kernel es una matriz bidimencional que se utliza para realizar enfoques, desenfoques, detección de bordes, realce, entre otras aplicaciones.\n",
    "\n",
    "\n",
    "Lo que nos es muy útil si lo que buscamos es obtener caracteristicas de una imagen.\n",
    "\n",
    "Para operar el kernel sobre una imagen es necesario realizar una convolución entre ambos.\n",
    "\n",
    "Recordando la definición de convolución continua es la siguiente:\n",
    "$$ s(t) = \\int x(a)w(t-a)\\,da $$\n",
    "\n",
    "$$ s(t) = (x * w)(t) $$\n",
    "\n",
    "Donde w es función de densidad de probabilidad, y w es 0 para los valores negativos\n",
    "\n",
    "Referente a nuestro problema, x es el ***input***, w es el ***kernel*** y s es lo llamado ***feauture map***\n",
    "\n",
    "Pero esta definición de convolución no es útil para nosotros ya que nuestros datos son discretos.\n",
    "\n",
    "Para esto existe la definición discreta de convolución:\n",
    "$$ s(t) = (x * w)(t) = \\sum_{a=-\\infty}^{\\infty} x(a)w(t-a)$$\n",
    "\n",
    "Aún sobre esto, en esta y muchas aplicaciones de machine learning el ***input*** es una matriz multidimensional de **datos**, a su vez el ***kernel*** es una matriz multidimensional de ***parámetros***, dicho esto, en terminología adaptada para los algoritmos de aprendizaje, a estas matrices multidimensionales se les llama ***tensors***, a su vez asuminos que su valor es 0 excepto en el conjunto finito de puntos que los conforman.\n",
    "\n",
    "La razón de esta última declaración es debido a que la convolución discreta está definda de $-\\infty\\ a\\ \\infty$ por lo que resultaría imposible operar con esta, sin definir el complemento del conjunto de puntos de los ***tensores***, debido a esto se definío al conjunto complemento como 0 en todos sus elemtos.\n",
    "\n",
    "Con esto en mente haremos uso de la convolución en más de una dimensión como ejes.\n",
    "\n",
    "Para ejemplificar usando el ejemplo anterior\n",
    "$$ S(i,j) = ( I * K)(i,j) = \\sum_{m} \\sum_{n} I(m,n)K(i-m,j-n) $$\n",
    "\n",
    "$$Aplicando\\ la\\ propiedad\\ conmutativa\\ de\\ la\\ convoución$$\n",
    "\n",
    "$$ S(i,j) = ( K * I)(i,j) = \\sum_{m} \\sum_{n} I(i-m,j-n)K(m,n) $$\n",
    "\n",
    "\n",
    "Usualmente la segunda forma es mayormente usada para la implemetación de las bibliotecas de ML, debido a que hay menos variación en el rango de valores válidos para m y n.\n",
    "\n",
    "La propieda conmutativa de la convolución surgío porque volteamos el kernel relativo al input, aunque esto únicamente tiene relevancia para la implementación, puesto que para el analisis de la CNN esto es irrelevante.\n",
    "\n",
    "Pese a esta propiedad, muchas otras bibliotecas hacen uso de una operación llamada ***cross-correlation*** la cual es parecida a la convolución, tanto que incluso es llamada de la mis forma, esta es:\n",
    "$$ S(i,j) = ( K * I)(i,j) = \\sum_{m} \\sum_{n} I(i+m,j+n)K(m,n) $$\n",
    "\n",
    "Para una representación gráfica del comportamiento de la convolución:\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"float: left;text-align: center;width: 50%;\"> <img src=\"conv1.png\" width=\"70%\"/>  </div>\n",
    "<div style=\"float: right;text-align: center;width: 50%;position: relative;\"> <img src=\"conv2.png\" width=\"80%\"/></div>\n",
    "</div>\n",
    "\n",
    "La convolución ofrece un menor número de operaciones y una ventaja respecto al espacio, ya que debido a que la interacción entre el ***input*** y el ***kernel*** es sumamente menor respecto a la clásica multiplicación de matrices, y debido a que kernels pequeños relativos al tamaño de la entrada son los utilizados el  costo respecto al almacenamiento es menor.\n",
    " \n",
    "\n",
    "### Interacción de la red ###\n",
    "\n",
    "En las CNN uno de los principios que se sobre pone a todo es *sparce interaction* o **escas iteraccion**, esto se ve reflejado en la interacción entre los nodos de las diferentes capas, ya que en estas la interacción *directa* que tiene la salida de un nodo $X_{n}^{l}$ donde *l* es la capa y *n* el nodo de la dicha capa, únicamente afectará **directamente** a los nodos $X_{n-1}^{l+1}$, $X_{n}^{l+1}$ y $X_{n+1}^{l+1}$ esto pensando que el kernel tiene un ***tamaño de 3***, esto promueve la escasa interacción.\n",
    "\n",
    "Pero debido a esto se podría a llegar a pensar \"¿Cómo obtienen toda la información que le ofrece la entrada si la interacción directa es tan poca?\", pues la solución a esto se plantea que se debe a la interacción **indirecta** de los nodos, ya que el **receptive field** crece conforme las capas se vuelvan mas profundas pudiendo tener interacción con todo los nodos.\n",
    "\n",
    "Para una representación gráfica del comportamiento de lo anterior:\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"float: left;text-align: center;width: 50%;\"> <img src=\"receptive_field.png\" width=\"50%\"/>  </div>\n",
    "<div style=\"float: right;text-align: center;width: 50%;\"> <img src=\"receptive_field2.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "Cabe mencionar que una de las mayores ventajas tanto de la convolución como de la escasa interacción es que ofrece una disminución en las dimensiones de los datos conforme pasa a través de las capas.\n",
    "\n",
    "### Parámetros compartidos ###\n",
    "En una red neuronal regular los ***parámetros de aprendizaje*** son llamados vectores de pesos, dando como resultdo una matriz de pesos por capa, donde esta matriz tiene un tamaño de $(W^{m * n})^{l}$, esto únicamente para la capa **l**, esto implica que existirán **L** matrices para realizar las operaciones pertinentes, el problema de esto recae en el número de usos que se le da a cada peso $w_{p,q}^{l}$, ya que únicamente tienen un uso para la generación de la salida respecto a los nodos de entrada.\n",
    "\n",
    "En las CNN esto es diferente ya que se hace uso de ***parameter sharing***, esto es debido a que en cada capa se hace uso de un kernel (filtro), para obtener las características necesarias, pero esto ofrece muchas ventajas entre las cuales está que únicamente se almacena una matriz mucho más pequeña que las anteriores.\n",
    "\n",
    "### Equivariance ###\n",
    "Debido al comportamiento particular que causa el ***parameter sharing*** en las CNN, las capas que las conforman desarrollan una propiedad llamada ***equivariance*** a las transformaciones.\n",
    "\n",
    "Que una función es *equivariance* significa que si el *input* cambia, el *output* cambia en la misma dirección. Específicamente una $f(x)$ es *equivariance* a una función g, si $f(g(x)) = g(f(x))$.\n",
    "\n",
    "En el sentido de la convolución si g es una función que transforma su input , f es la convulución, x el input y k el kernel, $g(f(x,k)) = f(g(x),k)$,\n",
    "$$ si Y =  g(X * K) = g(X) * K $$\n",
    "\n",
    "La convolución es parte únicamente de las primeras capas de las CNN, estas enfocadas a obtención de características\n",
    "\n",
    "## Pooling ##\n",
    "\n",
    "Para describir el comportamiento del ***pooling*** es sumamente útil, primeramente describir la composición típica de una capa en una CNN.\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"text-align: center;\"> <img src=\"poling.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "Una explicación breve sobre estas etapas sería la siguiente:\n",
    "+ Primera etapa: En esta estapa se realizan el conjunto de convoluciones a las entradas de forma paralela para producir un **conjunto de  activaciones lineares**\n",
    "+ Segunda etapa: En esta etapa cada activación lineal es pasada a través de una función de activación no-lineal, como la función de activación de rectificado lineal de su acrónimo en inglés (**ReLu**)\n",
    "    $$ ReLU, f(x) = x^{+} = max(0,x)  $$\n",
    "Una vez llegando a la tercera etapa, procederemos a explicar la función ***pooling***, esta función transforma la salida del proceso anterior en una ubicación determinada con un resumen de estadísticas de la salidas cercanas.\n",
    "\n",
    "Por ejemplo: \n",
    "El ***Max pooling*** en el resumen estadístico regresa el valor máximo de los vecinos de una área cuadrangular.\n",
    "\n",
    "Pero en lo general **pooling** ayuda a realizar una representación aproximada **invariante** a una transformación pequeña de lo que entro.\n",
    "\n",
    "Una transformación invariante significa, que si transformamos el input en una pequeña cantidad, muchos de los valores resultantes de la funcion no cambiaran\n",
    "\n",
    "Ya que es mas importante conocer las caracteristicas que la localización de ellas.\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"text-align: center;\"> <img src=\"pooling.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "falta...\n",
    "\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"text-align: center;\"> <img src=\"arch.png\" />  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
