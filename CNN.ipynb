{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "Las \"Convolutional Neural Networks - CNN\" o Redes Neuronales Convolucionales en español, son un tipo de red neuronal enfocada en el procesamieno de matrices bidimencionales, debido a esto son muy utlizadas para visión artifcial, clasificación y segmentación de imágenes.\n",
    "\n",
    "Las CNN, realizan en general el mismo funcionamiento de un \"Neural Network\", ya que primeramente realizan la extracción de características para posteriormente realizar el proceso de decisión.\n",
    "\n",
    "\"Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers\"\n",
    "\n",
    "## Convolución ##\n",
    "\n",
    "Lo relevante hablando de las CNN es como opera para realizar esto, pues el primer paso es realizar convoluciones.\n",
    "\n",
    "Para esto planteemos el ejemplo de imágenes.\n",
    "\n",
    "Cada imagen \"i\" fungirá como nuestra entrada, más precisamente $ I = \\{i_{1} ... i_{N} \\} $ .\n",
    "\n",
    "Una imagen es una matriz bidimensional, en donde cada uno de sus elementos cuenta con un valor del 0 al 255.\n",
    "\n",
    "Y a su vez tenemos un Kernel, tambien llamado filtro una vez lo operamos sobre toda la imagen.\n",
    "\n",
    "Un kernel es una matriz bidimencional que se utliza para realizar enfoques, desenfoques, detección de bordes, realce, entre otras aplicaciones.\n",
    "\n",
    "\n",
    "Lo que nos es muy útil si lo que buscamos es obtener caracteristicas de una imagen.\n",
    "\n",
    "Para operar el kernel sobre una imagen es necesario realizar una convolución entre ambos.\n",
    "\n",
    "Recordando la definición de convolución continua es la siguiente:\n",
    "$$ s(t) = \\int x(a)w(t-a)\\,da $$\n",
    "\n",
    "$$ s(t) = (x * w)(t) $$\n",
    "\n",
    "Donde w es función de densidad de probabilidad, y w es 0 para los valores negativos\n",
    "\n",
    "Referente a nuestro problema, x es el ***input***, w es el ***kernel*** y s es lo llamado ***feauture map***\n",
    "\n",
    "Pero esta definición de convolución no es útil para nosotros ya que nuestros datos son discretos.\n",
    "\n",
    "Para esto existe la definición discreta de convolución:\n",
    "$$ s(t) = (x * w)(t) = \\sum_{a=-\\infty}^{\\infty} x(a)w(t-a)$$\n",
    "\n",
    "Aún sobre esto, en esta y muchas aplicaciones de machine learning el ***input*** es una matriz multidimensional de **datos**, a su vez el ***kernel*** es una matriz multidimensional de ***parámetros***, dicho esto, en terminología adaptada para los algoritmos de aprendizaje, a estas matrices multidimensionales se les llama ***tensors***, a su vez asuminos que su valor es 0 excepto en el conjunto finito de puntos que los conforman.\n",
    "\n",
    "La razón de esta última declaración es debido a que la convolución discreta está definda de $-\\infty\\ a\\ \\infty$ por lo que resultaría imposible operar con esta, sin definir el complemento del conjunto de puntos de los ***tensores***, debido a esto se definío al conjunto complemento como 0 en todos sus elemtos.\n",
    "\n",
    "Con esto en mente haremos uso de la convolución en más de una dimensión como ejes.\n",
    "\n",
    "Para ejemplificar usando el ejemplo anterior\n",
    "$$ S(i,j) = ( I * K)(i,j) = \\sum_{m} \\sum_{n} I(m,n)K(i-m,j-n) $$\n",
    "\n",
    "$$Aplicando\\ la\\ propiedad\\ conmutativa\\ de\\ la\\ convoución$$\n",
    "\n",
    "$$ S(i,j) = ( K * I)(i,j) = \\sum_{m} \\sum_{n} I(i-m,j-n)K(m,n) $$\n",
    "\n",
    "\n",
    "Usualmente la segunda forma es mayormente usada para la implemetación de las bibliotecas de ML, debido a que hay menos variación en el rango de valores válidos para m y n.\n",
    "\n",
    "La propieda conmutativa de la convolución surgío porque volteamos el kernel relativo al input, aunque esto únicamente tiene relevancia para la implementación, puesto que para el analisis de la CNN esto es irrelevante.\n",
    "\n",
    "Pese a esta propiedad, muchas otras bibliotecas hacen uso de una operación llamada ***cross-correlation*** la cual es parecida a la convolución, tanto que incluso es llamada de la mis forma, esta es:\n",
    "$$ S(i,j) = ( K * I)(i,j) = \\sum_{m} \\sum_{n} I(i+m,j+n)K(m,n) $$\n",
    "\n",
    "Para una representación gráfica del comportamiento de la convolución:\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"float: left;text-align: center;width: 50%;\"> <img src=\"conv1.png\" width=\"70%\"/>  </div>\n",
    "<div style=\"float: right;text-align: center;width: 50%;position: relative;\"> <img src=\"conv2.png\" width=\"80%\"/></div>\n",
    "</div>\n",
    "\n",
    "La convolución ofrece un menor número de operaciones y una ventaja respecto al espacio, ya que debido a que la interacción entre el ***input*** y el ***kernel*** es sumamente menor respecto a la clásica multiplicación de matrices, y debido a que kernels pequeños relativos al tamaño de la entrada son los utilizados el  costo respecto al almacenamiento es menor.\n",
    " \n",
    "\n",
    "### Interacción de la red ###\n",
    "\n",
    "En las CNN uno de los principios que se sobre pone a todo es *sparce interaction* o **escas iteraccion**, esto se ve reflejado en la interacción entre los nodos de las diferentes capas, ya que en estas la interacción *directa* que tiene la salida de un nodo $X_{n}^{l}$ donde *l* es la capa y *n* el nodo de la dicha capa, únicamente afectará **directamente** a los nodos $X_{n-1}^{l+1}$, $X_{n}^{l+1}$ y $X_{n+1}^{l+1}$ esto pensando que el kernel tiene un ***tamaño de 3***, esto promueve la escasa interacción.\n",
    "\n",
    "Pero debido a esto se podría a llegar a pensar \"¿Cómo obtienen toda la información que le ofrece la entrada si la interacción directa es tan poca?\", pues la solución a esto se plantea que se debe a la interacción **indirecta** de los nodos, ya que el **receptive field** crece conforme las capas se vuelvan mas profundas pudiendo tener interacción con todo los nodos.\n",
    "\n",
    "Para una representación gráfica del comportamiento de lo anterior:\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"float: left;text-align: center;width: 50%;\"> <img src=\"receptive_field.png\" width=\"50%\"/>  </div>\n",
    "<div style=\"float: right;text-align: center;width: 50%;\"> <img src=\"receptive_field2.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "Cabe mencionar que una de las mayores ventajas tanto de la convolución como de la escasa interacción es que ofrece una disminución en las dimensiones de los datos conforme pasa a través de las capas.\n",
    "\n",
    "### Parámetros compartidos ###\n",
    "En una red neuronal regular los ***parámetros de aprendizaje*** son llamados vectores de pesos, dando como resultdo una matriz de pesos por capa, donde esta matriz tiene un tamaño de $(W^{m * n})^{l}$, esto únicamente para la capa **l**, esto implica que existirán **L** matrices para realizar las operaciones pertinentes, el problema de esto recae en el número de usos que se le da a cada peso $w_{p,q}^{l}$, ya que únicamente tienen un uso para la generación de la salida respecto a los nodos de entrada.\n",
    "\n",
    "En las CNN esto es diferente ya que se hace uso de ***parameter sharing***, esto es debido a que en cada capa se hace uso de un kernel (filtro), para obtener las características necesarias, pero esto ofrece muchas ventajas entre las cuales está que únicamente se almacena una matriz mucho más pequeña que las anteriores.\n",
    "\n",
    "### Equivariance ###\n",
    "Debido al comportamiento particular que causa el ***parameter sharing*** en las CNN, las capas que las conforman desarrollan una propiedad llamada ***equivariance*** a las transformaciones.\n",
    "\n",
    "Que una función es *equivariance* significa que si el *input* cambia, el *output* cambia en la misma dirección. Específicamente una $f(x)$ es *equivariance* a una función g, si $f(g(x)) = g(f(x))$.\n",
    "\n",
    "En el sentido de la convolución si g es una función que transforma su input , f es la convulución, x el input y k el kernel, $g(f(x,k)) = f(g(x),k)$,\n",
    "$$ si Y =  g(X * K) = g(X) * K $$\n",
    "\n",
    "La convolución es parte únicamente de las primeras capas de las CNN, estas enfocadas a obtención de características\n",
    "\n",
    "## Pooling ##\n",
    "\n",
    "Para describir el comportamiento del ***pooling*** es sumamente útil, primeramente describir la composición típica de una capa en una CNN.\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"text-align: center;\"> <img src=\"poling.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "Una explicación breve sobre estas etapas sería la siguiente:\n",
    "+ Primera etapa: En esta estapa se realizan el conjunto de convoluciones a las entradas de forma paralela para producir un **conjunto de  activaciones lineares**\n",
    "+ Segunda etapa: En esta etapa cada activación lineal es pasada a través de una función de activación no-lineal, como la función de activación de rectificado lineal de su acrónimo en inglés (**ReLu**)\n",
    "    $$ ReLU, f(x) = x^{+} = max(0,x)  $$\n",
    "Una vez llegando a la tercera etapa, procederemos a explicar la función ***pooling***, esta función transforma la salida del proceso anterior en una ubicación determinada con un resumen de estadísticas de la salidas cercanas.\n",
    "\n",
    "Por ejemplo: \n",
    "El ***Max pooling*** en el resumen estadístico regresa el valor máximo de los vecinos de una área cuadrangular.\n",
    "\n",
    "Pero en lo general **pooling** ayuda a realizar una representación aproximada **invariante** a una transformación pequeña de lo que entro.\n",
    "\n",
    "Una transformación invariante significa, que si transformamos el input en una pequeña cantidad, muchos de los valores resultantes de la funcion no cambiaran\n",
    "\n",
    "Ya que es mas importante conocer las características que la localización de ellas.\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"text-align: center;\"> <img src=\"pooling.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "Otra de las funciones detrás del ***poolling*** es cuando el número de parámetros de la siguiente capa está en función del tamaño de su entrada (como en el caso de que esa capa esté completamente conectada y basada en la multiplicación de matrices), ya que el poolling reducirá el tamaño de su entrada por lo que mejorará su eficiencia de procesamiento y reduce los requerimientos de memoria para almacenar los parámetros.\n",
    "\n",
    "A su vez el poolling funciona para ajustar los datos de entrada al tamaño necesario por las arquitecturas de la redes neuronales\n",
    "\n",
    "## Variante de función de Convolución ##\n",
    "\n",
    "Una de las razones de no usar la definición literaria de Convolución discreta es que será realizada en paralelo, ya que la convolución con un ***Kernel*** o filtro solo podría obtener un tipo de características, y usualmente querríamos obtener varias características en la misma capa.\n",
    "\n",
    "Otra razón es que usualmente los datos de entrada no serán matrices con valores reales sino que serán matrices con valores vectoriales, como podría ser una imagen a color donde cada elemento tendría un vector de 3 sub-elementos que describen la concentración de rojo, verde y azul.\n",
    "\n",
    "Y recordemos que una red convolucional tiene como entrada, en una capa oculta, el resultado de un conjunto de convoluciones, por lo que se necesitaría un sub-proceso para procesar una matriz con más dimensiones, si mantuvieramos la definción descrita anteriormente.\n",
    "\n",
    "Por lo que por ejemplo para trabajar con una imagen se concidera que tendremos como entrada y una salida un tensor 3-D, con un índice para indicar los canales y 2 índices para indicar la localización del pixel en la sub-imagen.\n",
    "\n",
    "Pensando en dar una explicación de como las bibliotecas computacionales funcionan, nos gustaría comentarles que estas trabajan con tensores 4-D, ya que ellos trabajon por lotes, es decir que el cuarto índice indica el lote en el que nos encontramos\n",
    "\n",
    "Otra de la cosas que nos debemos encargar al utilizar una variante de la función, es asegurarnos de que cuente con la propiedad conmutativa. Para que cuente con dicha propiedad, la entrada y la salida deben de tener el mismo número de canales.\n",
    "\n",
    "Para esto, asumimos que tenemos un tensor ***kernel*** 4-D **K** con elementos $K_{i,j,k,l}$ dando una conexión estrecha entre una unidad ***i*** en el canal de la salida y una unidad ***j*** en el canal de entrada, con un offset (tamaño de las filas y columnas del kernel) de ***k*** filas y de ***l*** columnas entre la unidad de salida y la de entrada. También tomamos como la entrada de datos a ***V*** como a sus elementos $V_{i,j,k}$ donde el canal es ***i***, las filas ***j*** y las columnas ***k***.Y consideramos la salida como ***Z*** con el mismo formato que **V**. \n",
    "\n",
    "Dado todo lo anterior la variante de la convlución es la siguiente:\n",
    "\n",
    "$$Z_{i,j,k} = \\sum_{l,m,n} V_{l,j+m-1,k+n-1}K_{i,l,m,n} $$\n",
    "\n",
    "\n",
    "Nota: el \"-1\" en la sumatoria surge porque el primer elemento de los arreglos está marcado como ***1***\n",
    "\n",
    "Si buscamos reducir el costo computacional de la operación arriesgandonos a no poder obtener las características, lo que se puede realizar es un downsampling, o submuestreo de la salida de convolución. Para esto únicamente guardamos los valores de cada ***s*** strides o pasos, esto se mostraría de la siguiente forma.\n",
    "\n",
    "$$Z_{i,j,k} = c(K,V,s)_{i,j,k} = \\sum_{l,m,n} [V_{l,(j-1)x s+m,(k-1)x s+n} K_{i,l,m,n}] $$\n",
    "\n",
    "otras variantes de la convolución podrían ser ***unshared convolution***:\n",
    "En esta variante no se hace uso tal cual de la convolución con un kernel, sino con una matriz de pesos en una ***MLP***:\n",
    "\n",
    "Para una matriz de pesos ***W*** 6-D, $W_{i,j,k,l,m,n}$, donde ***i*** es el canal de salida, ***j*** la fila de salida, ***k*** la columna de salida, ***l*** el canal de entrada, ***m*** el offset de filas respecto al input y ***n*** el offset de columnas respecto al input.\n",
    "\n",
    "$$ Z_{i,j,k} = \\sum_{l,m,n} [V_{l,j+m-1,k+n-1} W_{i,j,k,l,m,n}]   $$\n",
    "\n",
    "## Aprendizaje ##\n",
    "Para mejorar las predicciones, se debe realizar mejoras en el proceso de selección de características, por lo que esto implica mejorar los ***kernels*** utilizados, la razon de esto es que, como se comentó anteriormente, cada tipo de kernel está enfocado en obtener un tipo de características, esto realizando transformaciones de los datos, para de esta forma obtener una representación de los datos donde la característica buscada se haga presente de forma más evidente, también se puede abstraer como el proceso de eliminar el ruido de la información, para únicamente mantener la carcaterística buscada por el Kernel.\n",
    "\n",
    "Un ejemplo de kernel sería el Kernel Gaussiano, el cual se pude ver el resultado de su aplicacion en la siguiente imagen:\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "<div style=\"text-align: center;\"> <img src=\"Gaus.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "Retomando el tema del proceso para mejorar la obtención de características, el cual consiste en modificar los kernels para refinarlos y de esta manera obtenerlos de forma más precisa, se obtiene el gradiente respecto a la salida.\n",
    "\n",
    "En muchos casos este gradiente se puede obtener usando directamente la operación de convolución, pero a su vez en muchos casos no, incluyendo el caso en el que se hace uso de ***stride***. Como recordaremos, la convolución es una operación lineal y esta puede ser descrita como una multiplicación matricial (Si primero modificamos la forma del tensor de entrada como un vector), la matriz debera de ser dispersa. \n",
    "\n",
    "Una vez con esta representación de vector o con el tensor y la función de convolución, se procede a realizar el procedimiento similar al de back propagation como en una red totalmente conectada.\n",
    "\n",
    "\n",
    "Para ejemplificar el proceso de obtención del gradiente, supondremos que queremos entrenar una CNN que incorpora ***stride*** con un conjunto de ***kernels*** representado por ***K***, aplicado a una imagen multicanal (Imagen a color) ***V*** con un stride *s* definido como $c(K,V,s)$, es decir esta operación:\n",
    "\n",
    "$$Z_{i,j,k} = c(K,V,s)_{i,j,k} = \\sum_{l,m,n} [V_{l,(j-1)x s+m,(k-1)x s+n} K_{i,l,m,n}] $$\n",
    "\n",
    "Ya que nosotros buscamos minimizar la función de costo en este caso $J(V,K)$, donde mediante back-propagation obtendremos un tensor ***G***, descrito de la siguiente forma.\n",
    "    $$ G_{i,j,k} = \\frac{\\partial}{\\partial Z_{i,j,k}} J(\\textbf{V},\\textbf{K}) $$\n",
    "\n",
    "Para entrenar la red necesitamos computar la siguiente derivada respecto los pesos de los kernels:\n",
    "\n",
    "$$ g(\\textbf{G},\\textbf{V},s)_{i,j,k,l} = \\frac{\\partial}{\\partial K_{i,j,k,l}} J(\\textbf{V},\\textbf{K}) = \\sum_{m,n} [G_{i,m,n} V_{j,(m-1)x s+k,(n-1)x s+l}]  $$\n",
    "\n",
    "Si esta capa no es la capa inferior de la red, necesitaremos calcular el gradiente con respecto a ***V*** para propagar el error hacia atrás (input). Para hacerlo, podemos usar la siguiente función:\n",
    "$$ h(\\textbf{K},\\textbf{G},s)_{i,j,k} = \\frac{\\partial}{\\partial V_{i,j,k}} J(\\textbf{V},\\textbf{K}) \\\\ = \\sum_{l,m \\\\ s.t. \\\\ (l-1)xs+m=j }\\ \\sum_{n,p \\\\ s.t. \\\\ (n-1)xs+p=k } \\sum_{q} K_{q,i,m,p} G_{q,l,n}$$\n",
    "\n",
    "Una vez descritas todas las capas y características anteriores como fueron la ***convolución***, el ***poolling***, las variantes de la convolución y la variante necesaria para aplicar back-propagation, finalmente podemos ilustrarles una arquitectura de una CNN sencilla.\n",
    "\n",
    "<div >\n",
    "<div style=\"text-align: center;\"> <img src=\"arch.png\" width=\"50%\"/>  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "[Ejemplo escrito usando Keras](./EG_Keras.ipynb)\n",
    "[Ejemplo escrito usando PyTorch](./EG_Pytorch.ipynb)\n",
    "\n",
    "## Comparación PYTORCH Y KERAS. ##\n",
    "Como podrán observar se realizó el desarrollaron dos ejemplos de clasificación de imágenes, el objetivo era identificar entre perros y gatos, uno de los ejemplos fue realizado con Keras y el otro con Pytorch, así que tal vez se pregunten ¿Qué API debo usar yo? Vamos a responder esta pregunta más adelante. Antes hay que conocer un poco más sobre Keras y Pythorch para poder responder. \n",
    "\n",
    "\n",
    "### ¿Qué es Keras? ### \n",
    " Keras es una biblioteca de código abierto escrita en python lanzada en 2015. Su objetivo es acelerar la creación de redes neuronales: para ello, Keras no funciona como un framework independiente, sino como una interfaz de uso intuitivo (API) que permite acceder a varios frameworks de aprendizaje automático y desarrollarlos. Cabe aclarar que podemos considerar Keras como una API de alto nivel, esto facilita el desarrollo rápido y simple, es por eso que cuenta con gran popularidad. \n",
    " Algunos frameworks compatibles con keras son: Theano , Microsoft Cognitive Toolkit (anteriormente CNTK), TensorFlow y MXNet. \n",
    "\n",
    "\n",
    "### ¿Qué es Pytorch? ###\n",
    " Pytorch es una biblioteca de código abierto para python (también cuenta con una interfaz para c++) lanzada en  2017, además cuenta con el apoyo de Facebook.  Es una API de bajo nivel cuyo objetivo es el trabajo directo con expresiones de arreglos, esto a base de tensores. A pesar de eso y de su arquitectura compleja, Pytorch ha logrado conseguir popularidad en los últimos años y ahora cuenta con una gran comunidad que sigue creciendo. \n",
    " Una ventaja que es importante mencionar es que con Pytorch podemos tener mejor capacidad de depuración, es más directa y sin complicaciones, esto se debe a la integración que tiene con Python. Por último nos gustaría mencionar que tiene paralelismo de datos, esto significa que puede distribuir el trabajo computacional entre múltiples núcleos de CPU o GPU. \n",
    "\n",
    "\n",
    "### ¿Cuál es mejor? ###\n",
    "Para responder esta pregunta debemos tener en cuenta qué es lo que buscamos al implementar alguna biblioteca. Ambas opciones son muy buenas y traerán resultados similares, sin embargo, tener en cuenta las prioridades que tenemos es fundamental para tomar  una decisión. ¿Qué buscamos? \n",
    "\n",
    " Si buscas que la biblioteca tenga: bastante soporte, portabilidad, facilidad y rapidez en desarrollo, definitivamente es Keras.  Keras además es perfecta para principiantes, ya que no se necesita  invertir mucho tiempo  en detalles de implementación matemática. \n",
    "\n",
    " Pythorch es perfecto si es que tienes un dataset muy grande, ya que es bastante rápido y tiene un alto rendimiento. Además nos da más libertad para escribir capas personalizadas, lo cual lo hace más flexible. Por último, Pytorch tiene una implementación más concisa y legible,  nos permite saber más sobre cómo está funcionando todo.\n",
    "\n",
    " En nuestro caso, a pesar de que obtuvimos buenos resultados con ambas bibliotecas, Pytorch ganó un punto más porque nuestro dataset era muy grande y fue mucho más rápido el desarrollo con esta herramienta. Sin embargo consideraría recomendar más Keras para introducir al mundo del machine learning y deep learning. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
